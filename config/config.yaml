tiramisu:
    tiramisu_path: "/scratch/dl5133/Env/tiramisu" 
    env_type:  "model"
    tags_model_weights: "/scratch/dl5133/Dev/RL-Agent/tiramisu-env/env_api/scheduler/models/model_release_version.pt"

dataset:
    path: '/scratch/dl5133/Dev/RL-Agent/rl_autoscheduler/utils/program_generator/Dataset_multi/'
    offline : '/scratch/dl5133/Dev/RL-Agent/tiramisu-env/datasets/merged_valid_programs_40k.pkl'
    save_path: '/scratch/dl5133/Dev/RL-Agent/tiramisu-env/datasets'
    # When doing evaluation on the benchmark set the value to True
    is_benchmark: False
    benchmark_cpp_files: '/scratch/dl5133/Dev/RL-Agent/tiramisu-env/benchmark/'
    benchmark_path: '/scratch/dl5133/Dev/RL-Agent/tiramisu-env/datasets/benchmark_P0.pkl'

ray:
    results: "/scratch/dl5133/Dev/RL-Agent/tiramisu-env/ray_results"
    restore_checkpoint: "/scratch/dl5133/Dev/RL-Agent/tiramisu-env/ray_results/LSTM-1layer-cell_256-1_action_embedded-0_entropycoeff-BS_constraints_strict/PPO_TiramisuRlEnv_f1c62_00000_0_2023-04-11_04-36-49/checkpoint_001000"

experiment:
    name: "LSTM-1layer-cell_256-1_action_embedded-0_entropycoeff-BS_constraints_strict"
    checkpoint_frequency: 10
    checkpoint_num_to_keep: 20
    # Stop parameters : The following 3 values are the values to stop the experiment if any of them is reached 
    training_iteration: 1000
    timesteps_total: 2000000
    episode_reward_mean: 2
    # Use this value to punish or tolerate illegal actions from being taken
    legality_speedup: 1.0
    entropy_coeff: 0.0
    # Training parameters
    train_batch_size: 1024

policy_network:
    # Set this to True if you want to use shared weights between policy and value function
    vf_share_layers: False
    policy_hidden_layers: 
        - 2048
        - 512 
        - 64
    # If vf_share_layers is true then, these values won't be taken for the value network
    vf_hidden_layers: 
        - 512 
        - 64
    dropout_rate: 0.2
    lr: 0.00005